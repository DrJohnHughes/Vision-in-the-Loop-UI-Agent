{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75544d95-1bde-4892-9002-52e1e5621401",
   "metadata": {},
   "source": [
    "# Vision-in-the-Loop Embodied UI Agent\n",
    "## Redistricting Example\n",
    "#### Dr John Hughes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fef8c-d0b9-4618-8fad-21c676d9a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow \"from src....\" imports when running this notebook\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9d3dc-bdf3-4eb5-b0ad-23dfc085bd84",
   "metadata": {},
   "source": [
    "## Example Small ReAct-Style Loop \n",
    "#### Demonstrates capabilities of natural language querying of visual data\n",
    "- Sends a map screenshot or PDF page to LLaMA 3.2-Vision in Ollama\n",
    "- Asks for structured JSON output describing districts\n",
    "- Asks for a plain summary focusing on the referenced region\n",
    "\n",
    "##### ReAct Loop\n",
    "- Vision Language Model parses visual data → you refine with natural language commands → the model reasons again\n",
    "- CAn add calls to external tools such as GEoPandas or QGIS for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d9210-3c1f-4c20-8a67-0e5f598f1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1 - send an image of a redistricting map\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2-vision\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"You are an assistant that reads redistricting maps. \"\n",
    "            \"Extract any district names, numbers, and boundary notes. \"\n",
    "            \"Output in strict JSON format with keys: district, description.\"\n",
    "        ),\n",
    "        \"images\": [\"images/redistricting_map.png\"]  # path to local image\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"Raw Model Output:\\n\", response[\"message\"][\"content\"])\n",
    "\n",
    "# Step 2 - feed that back for regional focus\n",
    "structured_data = response[\"message\"][\"content\"]\n",
    "\n",
    "follow_up = ollama.chat(\n",
    "    model=\"llama3.2-vision\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Here is extracted redistricting data:\\n{structured_data}\\n\\n\"\n",
    "            \"Summarize only the impact on Glendale and Burbank communities.\"\n",
    "        )\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"\\nRegional Summary:\\n\", follow_up[\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpt-lab)",
   "language": "python",
   "name": "gpt-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
